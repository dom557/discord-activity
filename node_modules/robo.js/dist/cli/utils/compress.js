import fs from 'node:fs/promises';
import { createWriteStream, createReadStream } from 'node:fs';
import path from 'node:path';
import zlib from 'node:zlib';
import { logger } from '../../core/logger.js';
import { pipeline } from 'node:stream/promises';
import { packageJson } from './utils.js';

async function compressDirectory(inputDir, outputFile, ignore = []) {
  logger.debug(`Compressing ${inputDir} into ${outputFile}...`);
  const fileMetadatas = await collectFileMetadatas(inputDir, inputDir, ignore);
  const archiveMetadata = {
    files: fileMetadatas,
    nodeVersion: process.version,
    roboVersion: "v" + packageJson.version
  };
  logger.debug(`Compressing ${fileMetadatas.length} files...`);
  const outputDir = path.dirname(outputFile);
  try {
    await fs.access(outputDir);
  } catch {
    await fs.mkdir(outputDir, { recursive: true });
  }
  const tempDir = outputFile.slice(0, outputFile.lastIndexOf("."));
  logger.debug(`Creating temporary directory at`, tempDir);
  await fs.mkdir(tempDir, { recursive: true });
  const tempFilePaths = {};
  try {
    await Promise.all(
      fileMetadatas.map(async (metadata) => {
        try {
          logger.debug(`Compressing ${metadata.size} bytes of data for ${metadata.filePath}...`);
          let cleanFileName = metadata.filePath.replaceAll(path.sep, "-");
          if (cleanFileName.startsWith("-")) {
            cleanFileName = cleanFileName.slice(1);
          }
          const tempFilePath = path.join(tempDir, cleanFileName + ".tmp");
          logger.debug(`Writing compressed data to`, tempFilePath);
          await compressFile(metadata.filePath, tempFilePath);
          tempFilePaths[metadata.filePath] = tempFilePath;
          const stats = await fs.stat(tempFilePath);
          metadata.compressedSize = stats.size;
          logger.debug(`Compressed ${metadata.size} bytes of data into ${metadata.compressedSize} bytes`);
        } catch (e) {
          logger.error(`Failed to compress ${metadata.filePath}:`, e);
          throw e;
        }
      })
    );
    const writeStream = createWriteStream(outputFile);
    const metadataString = JSON.stringify(archiveMetadata);
    const metadataLength = Buffer.byteLength(metadataString);
    const metadataLengthBuffer = Buffer.alloc(4);
    metadataLengthBuffer.writeInt32BE(metadataLength);
    writeStream.write(metadataLengthBuffer);
    writeStream.write(metadataString);
    logger.debug(`Wrote ${metadataLength} bytes of metadata to the archive`);
    for (const metadata of fileMetadatas) {
      const readStream = createReadStream(tempFilePaths[metadata.filePath]);
      for await (const chunk of readStream) {
        writeStream.write(chunk);
      }
    }
    writeStream.end();
  } finally {
    logger.debug(`Finished compressing ${fileMetadatas.length} files into ${outputFile}`);
  }
  await Promise.all(Object.values(tempFilePaths).map((tempFilePath) => fs.unlink(tempFilePath)));
  await fs.rm(tempDir, { force: true, recursive: true });
}
async function compressFile(filePath, outputFile) {
  const readStream = createReadStream(path.join(process.cwd(), filePath));
  const writeStream = createWriteStream(outputFile);
  const brotli = zlib.createBrotliCompress();
  await pipeline(readStream, brotli, writeStream);
}
async function collectFileMetadatas(dirPath, baseDir, ignore) {
  let results = [];
  const entries = await fs.readdir(dirPath, { withFileTypes: true });
  for (const entry of entries) {
    const fullPath = path.join(dirPath, entry.name);
    const relativePath = path.relative(baseDir, fullPath);
    if (ignore.some((i) => relativePath === i || relativePath.startsWith(i + path.sep))) {
      continue;
    }
    if (entry.isDirectory()) {
      const subdirResults = await collectFileMetadatas(fullPath, baseDir, ignore);
      results = results.concat(subdirResults);
    } else if (entry.isFile()) {
      const stats = await fs.stat(fullPath);
      results.push({ filePath: fullPath.replace(process.cwd(), ""), size: stats.size });
    }
  }
  return results;
}

export { compressDirectory };
